{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fafddf3-78cc-45ca-9c47-13335faf0708",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/WAVE/apps/el8/conda/envs/JupyterHub/20231011-CUDA/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/WAVE/users/unix/smalladi/.conda/envs/venv/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/WAVE/users/unix/smalladi/.conda/envs/venv/lib/python3.9/site-packages/torchvision/image.so: undefined symbol: _ZN3c104cuda20CUDACachingAllocator9allocatorE'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1+cu117\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import logging\n",
    "import os\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import sys\n",
    "import tempfile\n",
    "import time\n",
    "import ignite\n",
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from monai.config import print_config\n",
    "from monai.data import (\n",
    "decollate_batch,\n",
    "DataLoader,\n",
    "CacheDataset\n",
    ")\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai.losses import DiceCELoss, DeepSupervisionLoss, DiceLoss\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.networks.nets import SegResNet, SegResNetDS, SwinUNETR\n",
    "from monai.transforms import (\n",
    "    Activations,\n",
    "    AsDiscrete,\n",
    "    EnsureChannelFirstd,\n",
    "    Compose,\n",
    "    LoadImaged,\n",
    "    Orientationd,\n",
    "    Spacingd,\n",
    "    CropForegroundd,\n",
    "    RandCropByPosNegLabeld,\n",
    "    ScaleIntensityRanged,\n",
    "    RandShiftIntensityd,\n",
    "    RandAffined,\n",
    "    RandFlipd,\n",
    "    SaveImaged,\n",
    "    ToTensord,\n",
    ")\n",
    "from monai.utils.misc import ImageMetaKey\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1eec704c-e278-40ba-baff-6c027bb56b3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f6d870e-73a4-4e93-a435-e79055940c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "524\n",
      "524\n",
      "524\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "524"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_dir = 'metrics/10-11/'\n",
    "root_dir = '/WAVE/users/unix/smalladi/varian_ml/Hecktor22'\n",
    "data_dir = 'hecktor2022_training/hecktor2022'\n",
    "resampled_ct_path = '/WAVE/users/unix/smalladi/varian_ml/hecktor2022_training/resampled/imagesTr'\n",
    "resampled_pt_path = '/WAVE/users/unix/smalladi/varian_ml/hecktor2022_training/resampled/imagesTr'\n",
    "resampled_label_path = '/WAVE/users/unix/smalladi/varian_ml/hecktor2022_training/resampled'\n",
    "\n",
    "train_images = sorted(\n",
    "    glob.glob(os.path.join(resampled_ct_path, \"*_CT*\")))\n",
    "train_images2 = sorted(\n",
    "    glob.glob(os.path.join(resampled_pt_path, \"*_PT*\")))\n",
    "train_labels = sorted(\n",
    "    glob.glob(os.path.join(resampled_label_path, \"labelsTr\", \"*.nii.gz\")))\n",
    "data_dicts = [{\"image\": image_name, \"image2\": pet_image, 'label': label_name}\n",
    "    for image_name, pet_image, label_name in zip(train_images, train_images2, train_labels)\n",
    "]\n",
    "# temp = [{'image': '/WAVE/users/unix/smalladi/varian_ml/hecktor2022_training/hecktor2022/resampled_largerCt/MDA-184__CT_nobrain.nii.gz', 'image2': '/WAVE/users/unix/smalladi/varian_ml/hecktor2022_training/hecktor2022/resampled_largerPt/MDA-184__PT_nobrain.nii.gz', 'label':'/WAVE/users/unix/smalladi/varian_ml/hecktor2022_training/hecktor2022/resampled_largerlabel/MDA-184__CT_nobrain.nii.gz'}]\n",
    "# data_dicts.remove(temp[0])\n",
    "print(len(train_images))\n",
    "print(len(train_images2))\n",
    "print(len(train_labels))\n",
    "len(data_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22b801e7-17ba-4b05-a87f-1756f10bf9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "x=[i for i in range(524)]\n",
    "# print(x)\n",
    "# random.shuffle(x)\n",
    "# print(x)\n",
    "train_index,val_index,test_index=x[:470],x[470:510],x[510:]\n",
    "train_files=[]\n",
    "val_files=[]\n",
    "test_files=[]\n",
    "for i in train_index:\n",
    "    train_files.append(data_dicts[i])\n",
    "for i in val_index:\n",
    "    val_files.append(data_dicts[i])\n",
    "for i in test_index:\n",
    "    test_files.append(data_dicts[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4c2e39a-71be-4db2-852e-e1b4bb47a6b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "470\n",
      "40\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "print(len(train_files))\n",
    "print(len(val_files))\n",
    "print(len(test_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ecf94650-54c4-4ff1-93b1-aa1347c1189d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "t_files = train_files[:20]\n",
    "v_files = val_files[:10]\n",
    "print(len(v_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b352e436-5d6e-48d3-8d7d-083e75efd6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_a_min = -200\n",
    "ct_a_max = 400\n",
    "pt_a_min = 0\n",
    "pt_a_max = 25\n",
    "crop_samples = 2\n",
    "input_size = [96, 96, 96]\n",
    "modes_2d = ['bilinear', 'bilinear', 'nearest']\n",
    "p = 0.5\n",
    "strength = 1\n",
    "image_keys = [\"image\", \"image2\", \"label\"]\n",
    "\n",
    "train_transforms = Compose([\n",
    "    LoadImaged(keys=[\"image\", \"image2\", \"label\"]),\n",
    "    EnsureChannelFirstd(keys = [\"image\", \"image2\", \"label\"]),\n",
    "    Orientationd(keys=[\"image\", \"image2\", \"label\"], axcodes=\"RAS\"),\n",
    "    # Spacingd(\n",
    "    #     keys=image_keys,\n",
    "    #     pixdim=(1, 1, 1),\n",
    "    #     mode=modes_2d,\n",
    "    # ),\n",
    "    ScaleIntensityRanged(keys=['image'], a_min=ct_a_min, a_max=ct_a_max, b_min=0.0, b_max=1.0, clip=True),\n",
    "    ScaleIntensityRanged(keys=['image2'], a_min=pt_a_min, a_max=pt_a_max, b_min=0.0, b_max=1.0, clip=True),\n",
    "    CropForegroundd(keys=image_keys, source_key='image'),\n",
    "    RandCropByPosNegLabeld(\n",
    "        keys=image_keys,\n",
    "        label_key='label',\n",
    "        spatial_size=input_size,\n",
    "        pos=1,\n",
    "        neg=1,\n",
    "        num_samples=crop_samples,\n",
    "        image_key='image',\n",
    "        image_threshold=0,\n",
    "    ),\n",
    "    RandAffined(keys=image_keys, prob=p,\n",
    "                    translate_range=(round(10 * strength), round(10 * strength), round(10 * strength)),\n",
    "                    padding_mode='border', mode=modes_2d),\n",
    "    RandAffined(keys=image_keys, prob=p, scale_range=(0.10 * strength, 0.10 * strength, 0.10 * strength),\n",
    "                    padding_mode='border', mode=modes_2d),\n",
    "    RandFlipd(keys=[\"image\", \"image2\", \"label\"], prob=p/3, spatial_axis=0),\n",
    "    RandFlipd(keys=[\"image\", \"image2\", \"label\"], prob=p/3, spatial_axis=1),\n",
    "    RandFlipd(keys=[\"image\", \"image2\", \"label\"], prob=p/3, spatial_axis=2),\n",
    "    RandShiftIntensityd(\n",
    "            keys=[\"image\", \"image2\"],\n",
    "            offsets=0.10,\n",
    "            prob=p,\n",
    "        ),\n",
    "    ToTensord(keys=[\"image\", \"image2\", \"label\"])\n",
    "])\n",
    "val_transforms = Compose([\n",
    "    LoadImaged(keys=[\"image\", \"image2\", \"label\"]),\n",
    "    EnsureChannelFirstd(keys = [\"image\", \"image2\", \"label\"]),\n",
    "    Orientationd(keys=[\"image\", \"image2\", \"label\"], axcodes=\"RAS\"),\n",
    "    # Spacingd(\n",
    "    #     keys=image_keys,\n",
    "    #     pixdim=(1, 1, 1),\n",
    "    #     mode=modes_2d,\n",
    "    # ),\n",
    "    ScaleIntensityRanged(keys=['image'], a_min=ct_a_min, a_max=ct_a_max, b_min=0.0, b_max=1.0, clip=True),\n",
    "    ScaleIntensityRanged(keys=['image2'], a_min=pt_a_min, a_max=pt_a_max, b_min=0.0, b_max=1.0, clip=True),\n",
    "    CropForegroundd(keys=image_keys, source_key='image'),\n",
    "    ToTensord(keys=[\"image\", \"image2\", \"label\"])\n",
    "])\n",
    "\n",
    "test_transforms = Compose([\n",
    "     LoadImaged(keys=[\"image\", \"image2\", \"label\"]),\n",
    "     EnsureChannelFirstd(keys = [\"image\", \"image2\", \"label\"]),\n",
    "        # Orientationd(keys=[\"image\", \"image2\", \"label\"], axcodes=\"RAS\"),\n",
    "        # Spacingd(\n",
    "        #     keys=image_keys,\n",
    "        #     pixdim=(1, 1, 1),\n",
    "        #     mode=modes_2d,\n",
    "        # ),\n",
    "       ScaleIntensityRanged(keys=['image'], a_min=ct_a_min, a_max=ct_a_max, b_min=0.0, b_max=1.0, clip=True),\n",
    "       ScaleIntensityRanged(keys=['image2'], a_min=pt_a_min, a_max=pt_a_max, b_min=0.0, b_max=1.0, clip=True),\n",
    "       CropForegroundd(keys=image_keys, source_key='image'),\n",
    "       ToTensord(keys=[\"image\", \"image2\", \"label\"])\n",
    "])\n",
    "\n",
    "    \n",
    "orig_transforms = Compose([\n",
    "    LoadImaged(keys=[\"image\", \"image2\", \"label\"])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3344eabf-9e12-46c3-bcfb-f7efb9579c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = CacheDataset(data=t_files, transform=train_transforms, cache_rate=0.0)\n",
    "train_loader = DataLoader(train_ds, batch_size=2, shuffle=True, num_workers=2)\n",
    "\n",
    "val_ds = CacheDataset(data=v_files, transform=val_transforms, cache_rate=0.01)\n",
    "val_loader = DataLoader(val_ds, batch_size=1, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2883d1a1-0e3e-4feb-8f98-c00553727611",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# train_ds = CacheDataset(data=train_files, transform=train_transforms, cache_rate=0.0)\n",
    "# train_loader = DataLoader(train_ds, batch_size=2, shuffle=True, num_workers=2)\n",
    "\n",
    "# val_ds = CacheDataset(data=val_files, transform=val_transforms, cache_rate=0.01)\n",
    "# val_loader = DataLoader(val_ds, batch_size=1, num_workers=2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fd5aad-3c64-482e-9470-8ab66d82179e",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = [{'image': '/WAVE/users/unix/smalladi/varian_ml/hecktor2022_training/hecktor2022/resampled_largerCt/MDA-184__CT_nobrain.nii.gz', 'image2': '/WAVE/users/unix/smalladi/varian_ml/hecktor2022_training/hecktor2022/resampled_largerPt/MDA-184__PT_nobrain.nii.gz', 'label':'/WAVE/users/unix/smalladi/varian_ml/hecktor2022_training/hecktor2022/resampled_largerlabel/MDA-184__CT_nobrain.nii.gz'}]\n",
    "temp1_ds = CacheDataset(data=temp, transform=train_transforms, cache_rate=0.0)\n",
    "temp1_loader = DataLoader(temp1_ds, batch_size=1, shuffle=True, num_workers=1)\n",
    "\n",
    "temp2_ds = CacheDataset(data=temp, transform=val_transforms, cache_rate=0.0)\n",
    "temp2_loader = DataLoader(temp2_ds, batch_size=1, num_workers=1)\n",
    "\n",
    "for data in temp2_loader:\n",
    "    key = 'label'\n",
    "    tensor = data[key]\n",
    "    count_0 = np.count_nonzero(tensor == 0)\n",
    "    count_1 = np.count_nonzero(tensor == 1)\n",
    "    count_2 = np.count_nonzero(tensor == 2)\n",
    "    count_3 = np.count_nonzero(tensor == 3)\n",
    "    \n",
    "    print(\"Count of 0's:\", count_0)\n",
    "    print(\"Count of 1's:\", count_1)\n",
    "    print(\"Count of 2's:\", count_2)\n",
    "    print(\"Count of 3's:\", count_3)\n",
    "    # print(tensor.shape)\n",
    "    # Define the values to check for (0, 1, and 2)\n",
    "    # values_to_check = [0, 1, 2]\n",
    "    \n",
    "    # Check if the tensor has any values other than the specified values\n",
    "    # has_other_values = not np.all(np.isin(tensor, values_to_check))\n",
    "    # if has_other_values:\n",
    "        # print(tensor.meta[ImageMetaKey.FILENAME_OR_OBJ])\n",
    "    unique_values = np.unique(tensor)\n",
    "    print(unique_values)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4788333-d114-45c5-91a8-ee53db0b39cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ckp(checkpoint_fpath, model, optimizer):\n",
    "    checkpoint = torch.load(checkpoint_fpath)\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    return (model,\n",
    "            optimizer,\n",
    "            checkpoint['epoch'],\n",
    "            checkpoint['best_metric'],\n",
    "            checkpoint['train_time'],\n",
    "            checkpoint['epoch_loss_values'],\n",
    "            checkpoint['metric_values'],\n",
    "            checkpoint['metric_values_1'],\n",
    "            checkpoint['metric_values_2'],\n",
    "            checkpoint['best_metric_fold'],\n",
    "            checkpoint['current_fold']\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4998610f-909b-42da-9a31-4346098068cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_interval = 1\n",
    "VAL_AMP = True\n",
    "lr = 0.0002\n",
    "momentum = 0\n",
    "sigmoid = False\n",
    "weight_decay = 1.0e-05\n",
    "T_0 = 40\n",
    "n_classes = 3\n",
    "n_channels = 2\n",
    "input_size = (96, 96, 96)\n",
    "max_epochs = 20\n",
    "device = torch.device(\"cuda:0\")\n",
    "model = SegResNet(\n",
    "    blocks_down=[1, 2, 2, 4],\n",
    "    init_filters=16,\n",
    "    blocks_up=[1, 1, 1],\n",
    "    in_channels = n_channels,\n",
    "    out_channels= n_classes,\n",
    "    dropout_prob = 0.2\n",
    ").to(device)\n",
    "\n",
    "# model = SegResNetDS(\n",
    "#   init_filters = 32,\n",
    "#   blocks_down = [1, 2, 2, 4, 4, 4],\n",
    "#   norm = 'BATCH',\n",
    "#   in_channels = n_channels,\n",
    "#   out_channels = n_classes,\n",
    "#   dsdepth = 4\n",
    "# ).to(device)\n",
    "\n",
    "\n",
    "# model = SwinUNETR(\n",
    "#         img_size=input_size,\n",
    "#         in_channels=n_channels,\n",
    "#         out_channels=n_classes,\n",
    "#         feature_size=48,\n",
    "#         use_checkpoint=True,\n",
    "#     ).to(device)\n",
    "\n",
    "# loss_function = DiceCELoss(to_onehot_y=True, softmax=True, include_background=False, sigmoid=False, squared_pred=True, smooth_nr=0,\n",
    "#   smooth_dr=1.0e-05)\n",
    "loss_function = DiceLoss(softmax=True, to_onehot_y=True)\n",
    "# loss_function = DeepSupervisionLoss(loss_function)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.0002,  weight_decay=1e-5)\n",
    "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=max_epochs)\n",
    "dice_metric = DiceMetric(include_background=False, reduction='mean', get_not_nans=False)\n",
    "dice_metric_batch = DiceMetric(include_background=False, reduction=\"mean_batch\", get_not_nans=False)\n",
    "post_label = AsDiscrete(to_onehot=n_classes)\n",
    "post_pred = AsDiscrete(argmax=True, to_onehot=n_classes)\n",
    "\n",
    "# use amp to accelerate training\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "# enable cuDNN benchmark\n",
    "torch.backends.cudnn.benchmark = True\n",
    "start_epoch = 0\n",
    "ckp_path = os.path.join(metrics_dir, 'checkpoint.pt')\n",
    "\n",
    "# define inference method\n",
    "def inference(input):\n",
    "    def _compute(input):\n",
    "        return sliding_window_inference(\n",
    "            inputs=input,\n",
    "            roi_size=input_size,\n",
    "            sw_batch_size=1,\n",
    "            predictor=model,\n",
    "            overlap=0.5,\n",
    "        )\n",
    "\n",
    "    if VAL_AMP:\n",
    "        with torch.cuda.amp.autocast():\n",
    "            return _compute(input)\n",
    "    else:\n",
    "        return _compute(input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cab173-ed60-4081-a5a1-c50931e36001",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "def save_ckp(state, is_best, metrics_dir):\n",
    "    f_path = os.path.join(metrics_dir, 'checkpoint.pt')\n",
    "    torch.save(state, f_path)\n",
    "    if is_best:\n",
    "        best_fpath = os.path.join(metrics_dir, 'best_model.pt')\n",
    "        shutil.copyfile(f_path, best_fpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c5a169-6c34-48be-8a40-a75ac8b5e5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_image(img_tensor, target_shape, affine, img_path):\n",
    "    test_output = torch.softmax(img_tensor, 1).cpu().numpy()\n",
    "    test_output = np.argmax(test_output, axis=1).astype(np.int16)[0]\n",
    "    test_output = resample_3d(test_output, target_shape)\n",
    "    nib.save(     \n",
    "            nib.Nifti1Image(test_output.astype(np.int16), affine), img_path\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ecb049-9354-435a-8895-1423c258a342",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_dicts = data_dicts[:10]\n",
    "print(d_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ecf1da-ae4b-4f77-b4cb-cf89804686e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "roi_size = (192, 192, 192)\n",
    "sw_batch_size = 2\n",
    "val_interval = 2\n",
    "best_metric = -1\n",
    "best_metric_epoch = -1\n",
    "epoch_loss_values = list()\n",
    "metric_values = list()\n",
    "metric_values_1 = list()\n",
    "metric_values_2 = list()\n",
    "best_metrics_epochs_and_time = [[], [], [], []]\n",
    "total_start = time.time()\n",
    "train_time = 0\n",
    "current_fold = 0\n",
    "best_metric_fold = 0\n",
    "\n",
    "# ckp_path = os.path.join(metrics_dir, 'checkpoint.pt')\n",
    "# (\n",
    "#     model,\n",
    "#     optimizer,\n",
    "#     start_epoch,\n",
    "#     best_metric,\n",
    "#     train_time,\n",
    "#     epoch_loss_values,\n",
    "#     metric_values,\n",
    "#     metric_values_1,\n",
    "#     metric_values_2,\n",
    "#     best_metric_fold,\n",
    "#     current_fold,\n",
    "#     ) = load_ckp(ckp_path, model, optimizer)\n",
    "\n",
    "max_epochs = 2\n",
    "folds = 3\n",
    "kfold = KFold(n_splits=folds, shuffle=True)\n",
    "\n",
    "for fold, (train_ids, test_ids) in enumerate(kfold.split(d_dicts)):\n",
    "    folder_name = f\"fold_{fold}\"\n",
    "    metric_values = list()\n",
    "    metric_values_1 = list()\n",
    "    metric_values_2 = list()\n",
    "    epoch_loss_values = list()\n",
    "\n",
    "    fold_start = time.time()\n",
    "    print(f'FOLD {fold}')\n",
    "    print('--------------------------------')\n",
    "    \n",
    "    train_files, val_files = [], []\n",
    "    for id in train_ids:\n",
    "        train_files.append(d_dicts[id])\n",
    "    for id in test_ids:\n",
    "        val_files.append(d_dicts[id])\n",
    "\n",
    "    # print(train_files, val_files)\n",
    "    # break\n",
    "    \n",
    "    train_ds = CacheDataset(data=train_files, transform=train_transforms, cache_rate=0.0)\n",
    "    train_loader = DataLoader(train_ds, batch_size=2, shuffle=True, num_workers=2)\n",
    "    \n",
    "    val_ds = CacheDataset(data=val_files, transform=val_transforms, cache_rate=0.01)\n",
    "    val_loader = DataLoader(val_ds, batch_size=1, num_workers=2)\n",
    "    \n",
    "    for epoch in range(start_epoch, max_epochs):\n",
    "        is_best = False\n",
    "        epoch_start = time.time()\n",
    "        print('-' * 10)\n",
    "        # print('Epoch {}/{}'.format(epoch + 1, max_epochs))\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        step = 0\n",
    "        for batch_data in train_loader:\n",
    "            step += 1\n",
    "            inputct, inputpt, labels = (\n",
    "                batch_data['image'].to(device),\n",
    "                batch_data['image2'].to(device), \n",
    "                batch_data['label'].to(device)\n",
    "            )\n",
    "            inputs = torch.concat([inputct, inputpt], axis=1)\n",
    "            optimizer.zero_grad()\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model(inputs)\n",
    "                loss = loss_function(outputs, labels)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            epoch_loss += loss.item()\n",
    "            # print(\n",
    "            #     f\"{step}/{len(train_ds) // train_loader.batch_size}\"\n",
    "            #     f\", train_loss: {loss.item():.4f}\"\n",
    "            # )\n",
    "        lr_scheduler.step()\n",
    "        epoch_loss /= step\n",
    "        epoch_loss_values.append(epoch_loss)\n",
    "        print(f\"epoch: {epoch + 1}/{max_epochs}, average loss: {epoch_loss:.4f}\")\n",
    "        model.eval()\n",
    "        image_no = 0\n",
    "        with torch.no_grad():\n",
    "            for val_data in val_loader:\n",
    "                image_no += 1\n",
    "                val_inputct, val_inputpt, val_label = (\n",
    "                    val_data[\"image\"].to(device),\n",
    "                    val_data[\"image2\"].to(device),\n",
    "                    val_data[\"label\"].to(device),\n",
    "                )\n",
    "                val_inputs = torch.concat([val_inputct, val_inputpt], axis=1)\n",
    "                val_outputs = sliding_window_inference(val_inputs, roi_size, sw_batch_size, model)\n",
    "                val_label_list = decollate_batch(val_label)\n",
    "                val_label_convert = [post_label(val_label_tensor) for val_label_tensor in val_label_list]\n",
    "                val_outputs_list = decollate_batch(val_outputs)\n",
    "                val_output_convert = [post_pred(val_pred_tensor) for val_pred_tensor in val_outputs_list]\n",
    "                value = dice_metric(y_pred=val_output_convert, y=val_label_convert)\n",
    "                dice_metric_batch(y_pred=val_output_convert, y=val_label_convert) \n",
    "                # print(\"Dice metrics:\", value[0][0], value[0][1])\n",
    "            metric = dice_metric.aggregate().item()\n",
    "            metric_values.append(metric)\n",
    "            metric_batch = dice_metric_batch.aggregate()\n",
    "            metric_1 = metric_batch[0].item()\n",
    "            metric_values_1.append(metric_1)\n",
    "            metric_2 = metric_batch[1].item()\n",
    "            metric_values_2.append(metric_2)\n",
    "            dice_metric.reset()\n",
    "            dice_metric_batch.reset()\n",
    "    \n",
    "            if metric > best_metric:\n",
    "                best_metric = metric\n",
    "                best_metric_epoch = epoch + 1\n",
    "                best_metric_fold = fold\n",
    "                best_metrics_epochs_and_time[0].append(best_metric)\n",
    "                best_metrics_epochs_and_time[1].append(best_metric_epoch)\n",
    "                best_metrics_epochs_and_time[2].append(time.time() - total_start)\n",
    "                best_metrics_epochs_and_time[3].append(fold)\n",
    "                is_best = True\n",
    "                print(\"saved new best metric model\")\n",
    "            print(\n",
    "                f\"current epoch: {epoch + 1} current mean dice: {metric:.4f}\"\n",
    "                f\" 1: {metric_1:.4f} 2: {metric_2:.4f}\"\n",
    "                f\"\\nbest mean dice: {best_metric:.4f}\"\n",
    "                f\" at epoch: {best_metric_epoch} and fold: {fold}\"\n",
    "            )\n",
    "        train_time += (time.time() - epoch_start)\n",
    "        checkpoint = {\n",
    "            'epoch': epoch + 1,\n",
    "            'best_metric': best_metric,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'train_time': train_time,\n",
    "            'epoch_loss_values': epoch_loss_values,\n",
    "            'metric_values': metric_values,\n",
    "            'metric_values_1': metric_values_1,\n",
    "            'metric_values_2': metric_values_2,\n",
    "            'best_metric_fold': best_metric_fold,\n",
    "            'current_fold': fold\n",
    "        }\n",
    "    \n",
    "        save_ckp(checkpoint, is_best, metrics_dir)\n",
    "        print(f\"time consuming of epoch {epoch + 1} is: {(time.time() - epoch_start):.4f}\")\n",
    "    fold_time = time.time() - fold_start\n",
    "    print(f\"Fold: {fold} completed, Fold time is: {fold_time}\")\n",
    "    \n",
    "    epoch_loss_values_df = pd.DataFrame(epoch_loss_values)\n",
    "    metric_values_df = pd.DataFrame(metric_values)\n",
    "    metric_values_1_df = pd.DataFrame(metric_values_1)\n",
    "    metric_values_2_df = pd.DataFrame(metric_values_2) \n",
    "    \n",
    "    epoch_loss_values_df.to_csv(os.path.join(metrics_dir, folder_name, \"epoch_loss_values.csv\"))\n",
    "    metric_values_df.to_csv(os.path.join(metrics_dir, folder_name, \"metric_values.csv\"))\n",
    "    metric_values_1_df.to_csv(os.path.join(metrics_dir, folder_name, \"metric_values_1.csv\"))\n",
    "    metric_values_2_df.to_csv(os.path.join(metrics_dir, folder_name, \"metric_values_2.csv\"))\n",
    "\n",
    "total_time = time.time() - total_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74bf44a-8ff4-45d9-a020-0609c90e6056",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"train completed, best_metric: {best_metric:.4f} at epoch: {best_metric_epoch}, total time: {total_time}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8355cd07-ead1-4f98-bc62-22e23271d6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_metrics_epochs_and_time_df = pd.DataFrame(best_metrics_epochs_and_time)\n",
    "epoch_loss_values_df = pd.DataFrame(epoch_loss_values)\n",
    "metric_values_df = pd.DataFrame(metric_values)\n",
    "metric_values_1_df = pd.DataFrame(metric_values_1)\n",
    "metric_values_2_df = pd.DataFrame(metric_values_2)\n",
    "best_metrics_epochs_and_time_df.to_csv(metrics_dir + \"best_metrics_epochs_and_time.csv\")\n",
    "epoch_loss_values_df.to_csv(metrics_dir + \"epoch_loss_values.csv\")\n",
    "metric_values_df.to_csv(metrics_dir + \"metric_values.csv\")\n",
    "metric_values_1_df.to_csv(metrics_dir + \"metric_values_1.csv\")\n",
    "metric_values_2_df.to_csv(metrics_dir + \"metric_values_2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6fbca2-b695-45c9-89f6-590936241358",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "metrics_dir = \"metrics/10-26_300\"\n",
    "epoch_loss_values = pd.read_csv(os.path.join(metrics_dir, \"epoch_loss_values.csv\"), header=None)\n",
    "# print(epoch_loss_values[1][1:])\n",
    "metric_values = pd.read_csv(os.path.join(metrics_dir, \"metric_values.csv\"), header=None)\n",
    "metric_values_1 = pd.read_csv(os.path.join(metrics_dir, \"metric_values_1.csv\"), header=None)\n",
    "metric_values_2 = pd.read_csv(os.path.join(metrics_dir, \"metric_values_2.csv\"), header=None)\n",
    "print(min(epoch_loss_values[1][1:]))\n",
    "print(max(metric_values[1][1:]))\n",
    "print(max(metric_values_1[1][1:]))\n",
    "print(max(metric_values_2[1][1:]))     \n",
    "plt.figure(\"train\", (12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Epoch Average Loss\")\n",
    "x = [i + 1 for i in range(len(epoch_loss_values) - 1)]\n",
    "y = epoch_loss_values[1][1:]\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.plot(x, y, color=\"red\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Val Mean Dice\")\n",
    "x = [i + 1 for i in range(len(metric_values) - 1)]\n",
    "y = metric_values[1][1:]\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.plot(x, y, color=\"green\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(\"train\", (12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Val Mean Dice : 1\")\n",
    "x = [i + 1 for i in range(len(metric_values_1) - 1)]\n",
    "y = metric_values_1[1][1:]\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.plot(x, y, color=\"blue\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Val Mean Dice : 2\")\n",
    "x = [i + 1 for i in range(len(metric_values_2) - 1)]\n",
    "y = metric_values_2[1][1:]\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.plot(x, y, color=\"brown\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7868765a-fff0-43c8-bf46-d272de375a38",
   "metadata": {},
   "source": [
    "#### Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902af13d-05ed-41e1-b9b6-cd3a94219c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = CacheDataset(data=test_files, transform=val_transforms, cache_rate=0.0)\n",
    "test_loader = DataLoader(test_ds, batch_size=1, num_workers=2)\n",
    "\n",
    "orig_ds = CacheDataset(data=test_files, transform=orig_transforms, cache_rate=0.0)\n",
    "orig_loader = DataLoader(orig_ds, batch_size=1, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eeaa0fe-d98c-436c-a96b-1c478788effa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.ndimage as ndimage\n",
    "def resample_3d(img, target_size):\n",
    "    imx, imy, imz = img.shape\n",
    "    tx, ty, tz = target_size\n",
    "    zoom_ratio = (float(tx) / float(imx), float(ty) / float(imy), float(tz) / float(imz))\n",
    "    img_resampled = ndimage.zoom(img, zoom_ratio, order=0, prefilter=False)\n",
    "    return img_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614f2bd6-e21b-423f-b4bc-2f9eeefaa3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_dir = \"metrics/10-26_300\"\n",
    "output_directory = 'data/output/10-26'\n",
    "slice_no = 90\n",
    "best_fpath = os.path.join(metrics_dir, \"best_model.pt\")\n",
    "best_model = torch.load(best_fpath)\n",
    "model.load_state_dict(best_model['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee0029a-dfdf-4ed0-ae77-bd097dfb7271",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "img  = 0\n",
    "roi_size = (192, 192, 192)\n",
    "sw_batch_size = 1\n",
    "for test, orig in zip(test_loader, orig_loader):\n",
    "    img += 1\n",
    "    img_name = \"test_pred_\" + str(img) + \".nii.gz\"\n",
    "    with torch.no_grad():\n",
    "        # select one image to evaluate and visualize the model output\n",
    "        test_inputct = test[\"image\"].to(device)\n",
    "        test_inputpt = test[\"image2\"].to(device)\n",
    "        test_label = test[\"label\"].to(device)\n",
    "        print(test_label.meta[ImageMetaKey.FILENAME_OR_OBJ])\n",
    "        _, _, h, w, d = test_label.shape\n",
    "        target_shape = (h, w, d)\n",
    "        test_affine = test[\"label_meta_dict\"][\"affine\"].numpy()[0]\n",
    "        print(test_affine)\n",
    "        test_input = torch.concat([test_inputct, test_inputpt], axis=1)\n",
    "        test_output = sliding_window_inference(test_input, roi_size, sw_batch_size, model)\n",
    "        labels = torch.argmax(test_output, dim=1)\n",
    "        labels = labels[0]\n",
    "        # test_label_list = decollate_batch(test_label)\n",
    "        # test_label_convert = [post_label(test_label_tensor) for test_label_tensor in test_label_list]\n",
    "        # test_output_list = decollate_batch(test_output)\n",
    "        # test_output_convert = [post_pred(test_output_tensor) for test_output_tensor in test_output_list]\n",
    "        # value = dice_metric(y_pred=test_output_convert, y=test_label_convert)\n",
    "        # value_batch = dice_metric_batch(y_pred=test_output_convert, y=test_label_convert) \n",
    "        # print(\"Dice metrics:\", dice_metric.aggregate().item())\n",
    "        # print(\"Dice metric batch:\", dice_metric_batch.aggregate()[0].item(), dice_metric_batch.aggregate()[1].item()) \n",
    "        # print(test_label_convert[0].shape, test_output_convert[0].shape)\n",
    "        # test_output_in = post_pred(test_output[0])\n",
    "        # test_output = torch.softmax(test_output, 1).cpu().numpy()\n",
    "        # test_output = np.argmax(test_output, axis=1).astype(np.int16)\n",
    "        # test_output = resample_3d(test_output, target_shape)\n",
    "        nib.save(     \n",
    "                nib.Nifti1Image(labels.astype(np.int16), test_affine), os.path.join(output_directory, img_name)\n",
    "            )\n",
    "    # plt.figure(\"label\", (6, 6))\n",
    "    # for i in range(1):\n",
    "    #     # plt.subplot(1, 1, i + 1)\n",
    "    #     plt.title(f\"label channel {i}\")\n",
    "    #     plt.imshow(test_label[0][i, :, :, slice_no].detach().cpu())\n",
    "    # plt.show()\n",
    "    # # visualize the 3 channels model output corresponding to this image\n",
    "    # plt.figure(\"output\", (18, 6))\n",
    "    # for i in range(3):\n",
    "    #     plt.subplot(1, 3, i + 1)\n",
    "    #     plt.title(f\"output channel {i}\")\n",
    "    #     plt.imshow(test_output_convert[0][i, :, :, slice_no].detach().cpu())\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2f77c8-b45f-4fce-88bf-e602c3951fc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88d1b6c-dbad-4321-8811-1a119fc224a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "img  = 0\n",
    "roi_size = (192, 192, 192)\n",
    "sw_batch_size = 1\n",
    "for test, orig in zip(test_loader, orig_loader):\n",
    "    img += 1\n",
    "    img_name = \"test_pred_\" + str(img) + \".nii.gz\"\n",
    "    with torch.no_grad():\n",
    "        # select one image to evaluate and visualize the model output\n",
    "        test_inputct = test[\"image\"].to(device)\n",
    "        test_inputpt = test[\"image2\"].to(device)\n",
    "        test_label = test[\"label\"].to(device)\n",
    "        affine = orig[\"label_meta_dict\"][\"affine\"].numpy()[0]\n",
    "        print(test_label.meta[ImageMetaKey.FILENAME_OR_OBJ])\n",
    "        # print(orig[\"label\"].meta[ImageMetaKey.FILENAME_OR_OBJ])\n",
    "        _, h, w, d = orig[\"label\"].shape\n",
    "        target_shape = (h, w, d)\n",
    "        # affine = test_ds[0][\"label_meta_dict\"][\"affine\"].numpy()\n",
    "        test_input = torch.concat([test_inputct, test_inputpt], axis=1)\n",
    "        test_output = sliding_window_inference(test_input, roi_size, sw_batch_size, model)\n",
    "        print(test_output.shape)\n",
    "        labels = torch.argmax(test_output, dim=1)\n",
    "        print(labels.shape)\n",
    "        labels = labels.permute(1,2,3,0)\n",
    "        print(labels.shape)\n",
    "        labels_array = labels.cpu().numpy()\n",
    "        nib.save(nib.Nifti1Image(labels_array.astype(np.int16), affine), 'labels.nii.gz')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f679ca05-0620-4c4d-a7a4-041bb0fa4475",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d436c269-48fe-417e-b093-8a355314193e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SegResNet(\n",
    "    blocks_down=[1, 2, 2, 4],\n",
    "    init_filters=16,\n",
    "    blocks_up=[1, 1, 1],\n",
    "    in_channels = n_channels,\n",
    "    out_channels= n_classes,\n",
    "    dropout_prob = 0.2\n",
    ").to(device)\n",
    "slice_no = 100\n",
    "img = 0\n",
    " \n",
    "model.load_state_dict(torch.load(os.path.join(metrics_dir, \"best_metric_model.pth\")))\n",
    "model.eval()\n",
    "for batch in test_loader:\n",
    "    img += 1\n",
    "    with torch.no_grad():\n",
    "        # select one image to evaluate and visualize the model output\n",
    "        test_inputct = batch[\"image\"].unsqueeze(0).to(device)\n",
    "        test_inputpt = batch[\"image2\"].unsqueeze(0).to(device)\n",
    "        test_input = torch.concat([test_inputct, test_inputpt], axis=1)\n",
    "        roi_size = (192, 192, 192)\n",
    "        sw_batch_size = 4\n",
    "        test_output = sliding_window_inference(test_input, roi_size, sw_batch_size, model)\n",
    "        test_output = post_pred(test_output[0])\n",
    "    plt.figure(\"label\", (6, 6))\n",
    "    for i in range(1):\n",
    "        # plt.subplot(1, 1, i + 1)\n",
    "        plt.title(f\"label channel {i}\")\n",
    "        plt.imshow(test_ds[image_no][\"label\"][i, :, :, slice_no].detach().cpu())\n",
    "    plt.show()\n",
    "    # visualize the 3 channels model output corresponding to this image\n",
    "    plt.figure(\"output\", (18, 6))\n",
    "    for i in range(3):\n",
    "        plt.subplot(1, 3, i + 1)\n",
    "        plt.title(f\"output channel {i}\")\n",
    "        plt.imshow(test_output[i, :, :, slice_no].detach().cpu())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19568af2-1add-4658-9da3-a59062382c1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
