{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca5c074-68d7-403c-82fe-4626ecf78f05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip -q install monai gdown einops mlflow pynrrd torchinfo \n",
    "!pip install pandas numpy nibabel tqdm\n",
    "!python -c \"import monai\" || pip install -q \"monai-weekly[gdown, nibabel, tqdm, ignite]\"\n",
    "!python -c \"import matplotlib\" || pip install -q matplotlib\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1015532b-b47f-4fc5-9b6d-b8969f3758f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import shutil\n",
    "import tempfile\n",
    "import time\n",
    "import warnings\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "from pathlib import Path\n",
    "from collections.abc import Callable, Sequence, Hashable\n",
    "from typing import Mapping,Dict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from monai.transforms import (\n",
    "    EnsureType,\n",
    "    FillHoles,\n",
    "    OneOf,\n",
    "    SpatialCropd,\n",
    "    Activations,\n",
    "    Activationsd,\n",
    "    ConcatItemsd,\n",
    "    AsDiscrete,\n",
    "    AsDiscreted,\n",
    "    Compose,\n",
    "    Invertd,\n",
    "    LoadImaged,\n",
    "    MapTransform,\n",
    "    RandAffined,\n",
    "    NormalizeIntensityd,\n",
    "    ToTensord,\n",
    "    EnsureChannelFirstd ,\n",
    "    Orientationd,\n",
    "    Spacingd,\n",
    "    ScaleIntensityRanged,\n",
    "    CropForegroundd,\n",
    "    NormalizeIntensityd,\n",
    "    Resized,\n",
    "    SaveImaged,\n",
    "    RandScaleIntensityd,\n",
    "    RandShiftIntensityd,\n",
    "    RandSpatialCropd,\n",
    "    RandSpatialCropSamplesd,\n",
    "    RandFlipd,\n",
    "    RandRotated,\n",
    "    EnsureTyped,\n",
    "    ScaleIntensityd,\n",
    "    RandCropByPosNegLabeld,\n",
    ")\n",
    "\n",
    "from monai.apps import DecathlonDataset\n",
    "from monai.handlers.utils import from_engine\n",
    "from monai.losses import DiceLoss, DiceCELoss\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai.metrics import DiceMetric, HausdorffDistanceMetric\n",
    "from monai.networks.nets import SegResNet\n",
    "from monai.data import Dataset, DataLoader, CacheDataset, decollate_batch\n",
    "from monai.utils import first\n",
    "from monai.utils import set_determinism\n",
    "from monai.config import print_config\n",
    "from monai.data.meta_tensor import MetaTensor\n",
    "from monai.config.type_definitions import NdarrayOrTensor\n",
    "from monai.utils.misc import ImageMetaKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba17af21-bd73-4cf9-b3d8-3e4047fecb86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "set_determinism(seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffdcf87-27e5-43ef-bba9-7d229a1a4207",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "root_dir = 'Hecktor22/model_data'\n",
    "data_dir = 'hecktor2022_training/hecktor2022'\n",
    "resampled_ct_path = 'hecktor2022_training/hecktor2022/resampled_largerCt'\n",
    "resampled_pt_path = 'hecktor2022_training/hecktor2022/resampled_largerPt'\n",
    "resampled_label_path = 'hecktor2022_training/hecktor2022/resampled_largerlabel'\n",
    "\n",
    "train_images = sorted(\n",
    "    glob(os.path.join(data_dir, \"imagesTr\", \"*_CT*\")))\n",
    "train_images2 = sorted(\n",
    "    glob(os.path.join(data_dir, \"imagesTr\", \"*_PT*\")))\n",
    "train_labels = sorted(\n",
    "    glob(os.path.join(data_dir, \"labelsTr\", \"*.nii.gz\")))\n",
    "data_dicts = [{\"image\": image_name, \"image2\": pet_image, 'label': label_name}\n",
    "    for image_name, pet_image, label_name in zip(train_images, train_images2, train_labels)\n",
    "]\n",
    "len(data_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9916f4-fe40-465d-be3c-69c951d16d07",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "root_dir = 'Hecktor22/model_data'\n",
    "data_dir = 'Hecktor22/data'\n",
    "\n",
    "train_images_ct = sorted(glob(os.path.join(data_dir, 'TrainData', '*_CT.nii.gz')))\n",
    "train_images_pt = sorted(glob(os.path.join(data_dir, 'TrainData', '*_PT.nii.gz')))\n",
    "train_labels = sorted(glob(os.path.join(data_dir, 'TrainLabels', '*.nii.gz')))\n",
    "train_files = [{\"image\": image_name, \"image2\": pet_image, 'label': label_name} for image_name, pet_image, label_name in zip(train_images_ct, train_images_pt, train_labels)]\n",
    "\n",
    "val_images_ct = sorted(glob(os.path.join(data_dir, 'ValData', '*_CT.nii.gz')))\n",
    "val_images_pt = sorted(glob(os.path.join(data_dir, 'ValData', '*_PT.nii.gz')))\n",
    "val_labels = sorted(glob(os.path.join(data_dir, 'ValLabels', '*.nii.gz')))\n",
    "val_files = [{\"image\": image_name, \"image2\": pet_image, 'label': label_name} for image_name, pet_image, label_name in zip(val_images_ct, val_images_pt, val_labels)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4434a5cd-a7f1-4101-bf05-a35972559531",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(len(train_files))\n",
    "print(len(train_images_ct))\n",
    "# print(len(train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167867b7-0da8-446f-90bc-9c5c61f9c6c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "folder_path = 'hecktor2022_training/hecktor2022/imagesTr'\n",
    "files_greater_than_60mb = []\n",
    "for root, dirs, files in os.walk(folder_path):\n",
    "    for file in files:\n",
    "        file_path = os.path.join(root, file)\n",
    "        file_size_mb = os.path.getsize(file_path) / (1024 * 1024)  # Convert file size to MB\n",
    "        if file_size_mb > 40:\n",
    "            files_greater_than_60mb.append(file_path)\n",
    "\n",
    "print(len(files_greater_than_60mb))      \n",
    "\n",
    "patients_greater_than_60mb = set()\n",
    "for file in files_greater_than_60mb:\n",
    "    file = file.split('/')[-1]\n",
    "    file = file.split('_')[0]\n",
    "    patients_greater_than_60mb.add(file)\n",
    "\n",
    "patients_greater_than_60mb = list(patients_greater_than_60mb)\n",
    "print(len(patients_greater_than_60mb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa188f2-3c96-42ff-b00c-9bfc2943cde6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "files = train_images\n",
    "substrings_to_remove = patients_greater_than_60mb\n",
    "\n",
    "filtered_files = [file for file in files if not any(substring in file for substring in substrings_to_remove)]\n",
    "\n",
    "train_images = filtered_files\n",
    "len(train_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f42941-7943-490d-8013-23c046437c4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "files = train_images2\n",
    "substrings_to_remove = patients_greater_than_60mb\n",
    "\n",
    "filtered_files = [file for file in files if not any(substring in file for substring in substrings_to_remove)]\n",
    "\n",
    "train_images2 = filtered_files\n",
    "len(train_images2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a204037-2ab0-49b5-a4dc-4717887bcbad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "files = train_labels\n",
    "substrings_to_remove = patients_greater_than_60mb\n",
    "\n",
    "filtered_files = [file for file in files if not any(substring in file for substring in substrings_to_remove)]\n",
    "\n",
    "train_labels = filtered_files\n",
    "len(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cbf304-3077-4de5-964d-78f7c4995849",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_dicts = [{\"image\": image_name, \"image2\": pet_image, 'label': label_name}\n",
    "    for image_name, pet_image, label_name in zip(train_images, train_images2, train_labels)\n",
    "]\n",
    "len(data_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1665020-267c-4da6-843b-2dad82c380d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "x=[i for i in range(294)]\n",
    "# print(x)\n",
    "random.shuffle(x)\n",
    "# print(x)\n",
    "train_index,val_index,test_index=x[:200],x[200:290],x[290:]\n",
    "train_files=[]\n",
    "val_files=[]\n",
    "test_files=[]\n",
    "for i in train_index:\n",
    "    train_files.append(data_dicts[i])\n",
    "for i in val_index:\n",
    "    val_files.append(data_dicts[i])\n",
    "for i in test_index:\n",
    "    test_files.append(data_dicts[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d4c43b-af4c-47c9-9352-1c157c091c20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(len(train_files))\n",
    "print(len(val_files))\n",
    "print(len(test_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49680af-b0e7-4939-9769-5d699445f8ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_files[22]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b523696e-6209-4b84-8aef-9f55c82989cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# root_dir = 'model_data'\n",
    "# data_dir = 'Hecktor22/data'\n",
    "\n",
    "# train_images_ct = sorted(glob(os.path.join(data_dir, 'TrainData', '*_CT.nii.gz')))\n",
    "# train_images_pt = sorted(glob(os.path.join(data_dir, 'TrainData', '*_PT.nii.gz')))\n",
    "# train_labels = sorted(glob(os.path.join(data_dir, 'TrainLabels', '*.nii.gz')))\n",
    "# train_files = [{\"image\": image_name, \"image2\": pet_image, 'label': label_name} for image_name, pet_image, label_name in zip(train_images_ct, train_images_pt, train_labels)]\n",
    "\n",
    "# val_images_ct = sorted(glob(os.path.join(data_dir, 'ValData', '*_CT.nii.gz')))\n",
    "# val_images_pt = sorted(glob(os.path.join(data_dir, 'ValData', '*_PT.nii.gz')))\n",
    "# val_labels = sorted(glob(os.path.join(data_dir, 'ValLabels', '*.nii.gz')))\n",
    "# val_files = [{\"image\": image_name, \"image2\": pet_image, 'label': label_name} for image_name, pet_image, label_name in zip(val_images_ct, val_images_pt, val_labels)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68953cd-2f3e-433d-8170-a86cf9b8148d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(train_files)\n",
    "# print(val_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59c80a3-86f7-4a1e-b5d7-4a1ee2c6a7d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class HecktorCropNeckRegion(CropForegroundd):\n",
    "    \"\"\"\n",
    "    A simple pre-processing transform to approximately crop the head and neck region based on a PET image.\n",
    "    This transform relies on several assumptions of patient orientation with a head location on the top,\n",
    "    and is specific for Hecktor22 dataset, and should not be used for an arbitrary PET image pre-processing.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        keys=[\"image\", \"image2\", \"label\"],\n",
    "        source_key=\"image\",\n",
    "        box_size=[200, 200, 310],\n",
    "        allow_missing_keys=True,\n",
    "        **kwargs,\n",
    "    ) -> None:\n",
    "        super().__init__(keys=keys, source_key=source_key, allow_missing_keys=allow_missing_keys, **kwargs)\n",
    "        self.box_size = box_size\n",
    "\n",
    "    def __call__(self, data : Mapping[Hashable, torch.Tensor]) -> Dict[Hashable, torch.Tensor]:\n",
    "\n",
    "        d = dict(data)\n",
    "        im_pet = d[\"image2\"][0]\n",
    "        #print(im_pet)\n",
    "        box_size = np.array(self.box_size)  # H&N region to crop in mm , defaults to 200x200x310mm\n",
    "        filename = \"\"\n",
    "\n",
    "        if isinstance(im_pet, MetaTensor):\n",
    "            filename = im_pet.meta[ImageMetaKey.FILENAME_OR_OBJ]\n",
    "            box_size = (box_size / np.array(im_pet.pixdim)).astype(int)  # compensate for resolution\n",
    "\n",
    "        box_start, box_end = self.extract_roi(im_pet=im_pet, box_size=box_size)\n",
    "        \n",
    "        if \"label\" in d and \"label\" in self.keys:\n",
    "            # if label mask is available, let's check if the cropped region includes all foreground\n",
    "            before_sum = d[\"label\"].sum().item()\n",
    "            after_sum = (\n",
    "                (d[\"label\"][0, box_start[0] : box_end[0], box_start[1] : box_end[1], box_start[2] : box_end[2]])\n",
    "                .sum()\n",
    "                .item()\n",
    "            )\n",
    "            if before_sum != after_sum:\n",
    "                print(\"WARNING, H&N crop could be incorrect!!!\", before_sum, after_sum)\n",
    "\n",
    "        d[self.start_coord_key] = box_start\n",
    "        d[self.end_coord_key] = box_end\n",
    "        \n",
    "        for key, m in self.key_iterator(d, self.mode): #question: what is mode in the iterators?\n",
    "            self.push_transform(d, key, extra_info={\"box_start\": box_start, \"box_end\": box_end})\n",
    "            d[key] = self.cropper.crop_pad(img=d[key], box_start=box_start, box_end=box_end, mode=m)\n",
    "        return d\n",
    "\n",
    "    def extract_roi(self, im_pet, box_size):\n",
    "\n",
    "        crop_len = int(0.75 * im_pet.shape[2])\n",
    "        im = im_pet[..., crop_len:]\n",
    "\n",
    "        mask = ((im - im.mean()) / im.std()) > 1\n",
    "        comp_idx = torch.argwhere(mask)\n",
    "        center = torch.mean(comp_idx.float(), dim=0).cpu().int().numpy()\n",
    "        xmin = torch.min(comp_idx, dim=0).values.cpu().int().numpy()\n",
    "        xmax = torch.max(comp_idx, dim=0).values.cpu().int().numpy()\n",
    "\n",
    "        xmin[:2] = center[:2] - box_size[:2] // 2\n",
    "        xmax[:2] = center[:2] + box_size[:2] // 2\n",
    "\n",
    "        xmax[2] = xmax[2] + crop_len\n",
    "        xmin[2] = max(0, xmax[2] - box_size[2])\n",
    "\n",
    "        return xmin.astype(int), xmax.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12e913f-b244-4b5c-a0bc-12f1f4d0c004",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    " class CalculateCentreOfMass(CropForegroundd):\n",
    "        def __init__(self, keys=[\"image\", \"image2\", \"label\"], source_key=\"image\", **kwargs):\n",
    "            super().__init__(keys=keys, source_key=source_key, **kwargs)\n",
    "        def __call__(self, data : Mapping[Hashable, torch.Tensor]):\n",
    "            d = dict(data)\n",
    "            #img_pet = d[\"image2\"][0]\n",
    "            #img_nib = img_pet #nib.load(\"C:/Users/Julia Scott/Documents/Varian_2022/Hecktor2022/Hecktor_test/data\\\\TrainData\\\\CHUM-007__PT.nii.gz\")\n",
    "            #image = img_nib\n",
    "            def CalculateCenterOfMass():\n",
    "                image_tensor = d[\"image2\"][0]\n",
    "                meshgrid = torch.meshgrid([torch.arange(image_size) for image_size in image_tensor.shape])\n",
    "                center_of_mass = torch.stack([torch.sum(image_tensor * meshgrid[dim]) / torch.sum(image_tensor) for dim in range(3)])\n",
    "                return center_of_mass.tolist()\n",
    "            SpatialCropd(keys=[\"image\", \"image2\", \"label\"],roi_center = CalculateCenterOfMass(), roi_size=[192, 192, 192])\n",
    "            return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74007bbf-8cfa-4a06-8b1b-a51290be0a08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ConvertToMultiChannelBasedOnClassesd(MapTransform):\n",
    "    def __call__(self, data):\n",
    "        d = dict(data)\n",
    "        for key in self.keys:\n",
    "            result = []\n",
    "            result.append(d[key] == 1)\n",
    "            result.append(d[key] == 2)\n",
    "            d[key] = torch.stack(result, axis=0).float()\n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f59112-4094-4b12-a1b4-609c8f4e6c1d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_transforms = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=['image', 'image2', 'label']),\n",
    "        EnsureChannelFirstd(keys = ['image', 'image2']),\n",
    "        EnsureTyped(keys=[\"image\", 'image2', \"label\"]),\n",
    "        Spacingd(\n",
    "            keys=[\"image\", 'image2'],\n",
    "            pixdim=(1.0, 1.0, 1.0),\n",
    "            mode=(\"bilinear\", \"nearest\"),\n",
    "        ),\n",
    "        ConvertToMultiChannelBasedOnClassesd(keys='label'),\n",
    "        HecktorCropNeckRegion(keys=[\"image\", 'image2', \"label\"], source_key=\"image\"),\n",
    "        RandSpatialCropd(keys=[\"image\", 'image2', \"label\"], roi_size=[192, 192, 192], random_size=False),\n",
    "        # CalculateCentreOfMass(keys=[\"image\", \"image2\", \"label\"], source_key=\"image\"),\n",
    "        Orientationd(keys=[\"image\", \"image2\"], axcodes=\"RAS\"),\n",
    "        ScaleIntensityd(keys=[\"image\", \"image2\"], minv=0.0, maxv=1.0),\n",
    "        NormalizeIntensityd(keys=\"image\", nonzero=True, channel_wise=True),\n",
    "        NormalizeIntensityd(keys=\"image2\", nonzero=True, channel_wise=True),\n",
    "        RandFlipd(keys=[\"image\", \"image2\", \"label\"], prob=0.5, spatial_axis=0),\n",
    "        RandFlipd(keys=[\"image\", \"image2\", \"label\"], prob=0.5, spatial_axis=1),\n",
    "        RandFlipd(keys=[\"image\", \"image2\", \"label\"], prob=0.5, spatial_axis=2),\n",
    "        ConcatItemsd(keys=[\"image\", \"image2\"], name=\"image_petct\", dim=0),\n",
    "        # RandRotated(keys=[\"image\", \"image2\", \"label\"], prob = 0.5),\n",
    "        # SaveImaged(\n",
    "        # keys = ['image', 'image2', 'label'],\n",
    "        # output_dir='data/output',\n",
    "        # output_postfix=\"crop\",\n",
    "        # resample=False,\n",
    "        # output_dtype=np.int16,\n",
    "        # separate_folder=False,\n",
    "        # )\n",
    "    ]\n",
    ")\n",
    "val_transforms = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=['image', 'image2', 'label']),\n",
    "        EnsureChannelFirstd(keys = ['image', 'image2']),\n",
    "        EnsureTyped(keys=[\"image\", 'image2', \"label\"]),\n",
    "        Spacingd(\n",
    "            keys=[\"image\", 'image2'],\n",
    "            pixdim=(1.0, 1.0, 1.0),\n",
    "            mode=(\"bilinear\", \"nearest\"),\n",
    "        ),\n",
    "        ConvertToMultiChannelBasedOnClassesd(keys='label'),\n",
    "        HecktorCropNeckRegion(keys=[\"image\", 'image2', \"label\"], source_key=\"image\"),\n",
    "        RandSpatialCropd(keys=[\"image\", 'image2', \"label\"], roi_size=[192, 192, 192], random_size=False),\n",
    "        # CalculateCentreOfMass(keys=[\"image\", \"image2\", \"label\"], source_key=\"image\"),\n",
    "        Orientationd(keys=[\"image\", \"image2\"], axcodes=\"RAS\"),\n",
    "        \n",
    "        ScaleIntensityd(keys=[\"image\", \"image2\"], minv=0.0, maxv=1.0),\n",
    "        NormalizeIntensityd(keys=\"image\", nonzero=True, channel_wise=True),\n",
    "        NormalizeIntensityd(keys=\"image2\", nonzero=True, channel_wise=True),\n",
    "        ConcatItemsd(keys=[\"image\", \"image2\"], name=\"image_petct\", dim=0),\n",
    "    ]\n",
    ")\n",
    "\n",
    "original_transforms = Compose(\n",
    "    [\n",
    "         LoadImaged(keys=['image', 'image2', 'label']),\n",
    "         EnsureChannelFirstd(['image', 'image2', 'label']),\n",
    "\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a56a0c-e6a0-4505-8ad9-11fbfa0ea604",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "check_ds = Dataset(data=train_files, transform=train_transforms)\n",
    "check_loader = DataLoader(check_ds, batch_size=1)\n",
    "for check_data in check_loader:\n",
    "    print(check_data['image_petct'].shape)\n",
    "    print(check_data['label'].shape)\n",
    "    break\n",
    "image, label = (check_data[\"image_petct\"][0][0], check_data[\"label\"][0][0])\n",
    "print(f\"image shape: {image.shape}, label shape: {label.shape}\")\n",
    "check_data = check_ds[0]\n",
    "plt.figure(\"image_petct\", (12, 6))\n",
    "for i in range(2):\n",
    "    plt.subplot(1, 2, i + 1)\n",
    "    plt.title(f\"image channel {i}\")\n",
    "    plt.imshow(check_data[\"image_petct\"][i, :, :, 150].detach().cpu())\n",
    "plt.show()\n",
    "plt.figure(\"label\", (12, 6))\n",
    "for i in range(2):\n",
    "    plt.subplot(1, 2, i + 1)\n",
    "    plt.title(f\"label channel {i}\")\n",
    "    plt.imshow(check_data[\"label\"][i, :, :, 150].detach().cpu())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28072e9c-8807-47be-a40b-53d17d295d57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_ds = CacheDataset(data=tfiles, transform=train_transforms, cache_rate=1.0, num_workers=4)\n",
    "train_loader = DataLoader(train_ds, batch_size=1, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d9d9d6-6918-49d8-ae5a-8da89f6e128e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = train_ds[0]\n",
    "print(data['image_petct'].shape)\n",
    "# print(data['image2'].shape)\n",
    "print(data['label'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66cfddd-e1c0-47a9-bbf6-88e3d5b2e338",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(data['image'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0fa0ef-4255-457f-a7e5-da93b91ee385",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_epochs = 2\n",
    "val_interval = 1\n",
    "VAL_AMP = True\n",
    "\n",
    "# standard PyTorch program style: create SegResNet, DiceLoss and Adam optimizer\n",
    "device = torch.device(\"cuda:0\")\n",
    "model = SegResNet(\n",
    "    blocks_down=[1, 2, 2, 4],\n",
    "    blocks_up=[1, 1, 1],\n",
    "    init_filters=16,\n",
    "    in_channels=2,\n",
    "    out_channels=2,\n",
    "    dropout_prob=0.2,\n",
    ").to(device)\n",
    "loss_function = DiceLoss(smooth_nr=0, smooth_dr=1e-5, squared_pred=True, to_onehot_y=False, sigmoid=True)\n",
    "optimizer = torch.optim.Adam(model.parameters(), 1e-4, weight_decay=1e-5)\n",
    "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=max_epochs)\n",
    "\n",
    "dice_metric = DiceMetric(include_background=True, reduction=\"mean\")\n",
    "dice_metric_batch = DiceMetric(include_background=True, reduction=\"mean_batch\")\n",
    "post_trans = Compose([Activations(sigmoid=True), AsDiscrete(threshold=0.5)])\n",
    "\n",
    "\n",
    "# define inference method\n",
    "def inference(input):\n",
    "    def _compute(input):\n",
    "        return sliding_window_inference(\n",
    "            inputs=input,\n",
    "            roi_size=(192, 192, 192),\n",
    "            sw_batch_size=1,\n",
    "            predictor=model,\n",
    "            overlap=0.5,\n",
    "        )\n",
    "\n",
    "    if VAL_AMP:\n",
    "        with torch.cuda.amp.autocast():\n",
    "            return _compute(input)\n",
    "    else:\n",
    "        return _compute(input)\n",
    "\n",
    "\n",
    "# use amp to accelerate training\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "# enable cuDNN benchmark\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9badcdc-631d-4069-a0d7-801776c3ffa8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_metric = -1\n",
    "best_metric_epoch = -1\n",
    "best_metrics_epochs_and_time = [[], [], []]\n",
    "epoch_loss_values = []\n",
    "metric_values = []\n",
    "metric_values_1 = []\n",
    "metric_values_2 = []\n",
    "\n",
    "total_start = time.time()\n",
    "for epoch in range(max_epochs):\n",
    "    epoch_start = time.time()\n",
    "    print(\"-\" * 10)\n",
    "    print(f\"epoch {epoch + 1}/{max_epochs}\")\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    step = 0\n",
    "    for batch_data in train_loader:\n",
    "        step_start = time.time()\n",
    "        step += 1\n",
    "        inputsct, inputspt, labels = (\n",
    "            batch_data[\"image\"].to(device),\n",
    "            batch_data[\"image2\"].to(device),\n",
    "            batch_data[\"label\"].to(device),\n",
    "        )\n",
    "        inputs = torch.concat([inputsct, inputspt], axis=1)\n",
    "        optimizer.zero_grad()\n",
    "        with torch.cuda.amp.autocast():\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_function(outputs, labels)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        epoch_loss += loss.item()\n",
    "        print(\n",
    "            f\"{step}/{len(train_ds) // train_loader.batch_size}\"\n",
    "            f\", train_loss: {loss.item():.4f}\"\n",
    "            f\", step time: {(time.time() - step_start):.4f}\"\n",
    "        )\n",
    "    lr_scheduler.step()\n",
    "    epoch_loss /= step\n",
    "    epoch_loss_values.append(epoch_loss)\n",
    "    print(f\"epoch {epoch + 1} average loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    if (epoch + 1) % val_interval == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for val_data in val_loader:\n",
    "                val_inputsct, val_inputspt, val_labels = (\n",
    "                    val_data[\"image\"].to(device),\n",
    "                    val_data[\"image2\"].to(device),\n",
    "                    val_data[\"label\"].to(device),\n",
    "                )\n",
    "                val_inputs = torch.concat([val_inputsct, val_inputspt], axis=1)\n",
    "                val_outputs = inference(val_inputs)\n",
    "                val_outputs = [post_trans(i) for i in decollate_batch(val_outputs)]\n",
    "                dice_metric(y_pred=val_outputs, y=val_labels)\n",
    "                dice_metric_batch(y_pred=val_outputs, y=val_labels)\n",
    "\n",
    "            metric = dice_metric.aggregate().item()\n",
    "            metric_values.append(metric)\n",
    "            metric_batch = dice_metric_batch.aggregate()\n",
    "            metric_1 = metric_batch[0].item()\n",
    "            metric_values_1.append(metric_1)\n",
    "            metric_2 = metric_batch[1].item()\n",
    "            metric_values_2.append(metric_2)\n",
    "            dice_metric.reset()\n",
    "            dice_metric_batch.reset()\n",
    "\n",
    "            if metric > best_metric:\n",
    "                best_metric = metric\n",
    "                best_metric_epoch = epoch + 1\n",
    "                best_metrics_epochs_and_time[0].append(best_metric)\n",
    "                best_metrics_epochs_and_time[1].append(best_metric_epoch)\n",
    "                best_metrics_epochs_and_time[2].append(time.time() - total_start)\n",
    "                torch.save(\n",
    "                    model.state_dict(),\n",
    "                    os.path.join(root_dir, \"best_metric_model.pth\"),\n",
    "                )\n",
    "                print(\"saved new best metric model\")\n",
    "            print(\n",
    "                f\"current epoch: {epoch + 1} current mean dice: {metric:.4f}\"\n",
    "                f\" 1: {metric_1:.4f} 2: {metric_2:.4f}\"\n",
    "                f\"\\nbest mean dice: {best_metric:.4f}\"\n",
    "                f\" at epoch: {best_metric_epoch}\"\n",
    "            )\n",
    "    print(f\"time consuming of epoch {epoch + 1} is: {(time.time() - epoch_start):.4f}\")\n",
    "total_time = time.time() - total_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a866a7e5-c6a3-4db4-a408-7c7c7668597c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_epochs = 10\n",
    "val_interval = 1\n",
    "VAL_AMP = True\n",
    "\n",
    "# standard PyTorch program style: create SegResNet, DiceLoss and Adam optimizer\n",
    "device = torch.device(\"cuda:0\")\n",
    "model = SegResNet(\n",
    "    blocks_down=[1, 2, 2, 4],\n",
    "    blocks_up=[1, 1, 1],\n",
    "    init_filters=16,\n",
    "    in_channels=2,\n",
    "    out_channels=2,\n",
    "    dropout_prob=0.2,\n",
    ").to(device)\n",
    "loss_function = DiceLoss(smooth_nr=0, smooth_dr=1e-5, squared_pred=True, to_onehot_y=False, sigmoid=True)\n",
    "optimizer = torch.optim.Adam(model.parameters(), 1e-4, weight_decay=1e-5)\n",
    "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=max_epochs)\n",
    "\n",
    "dice_metric = DiceMetric(include_background=True, reduction=\"mean\")\n",
    "dice_metric_batch = DiceMetric(include_background=True, reduction=\"mean_batch\")\n",
    "\n",
    "post_trans = Compose([Activations(sigmoid=True), AsDiscrete(threshold=0.5)])\n",
    "\n",
    "\n",
    "# define inference method\n",
    "def inference(input):\n",
    "    def _compute(input):\n",
    "        return sliding_window_inference(\n",
    "            inputs=input,\n",
    "            roi_size=(192, 192, 192),\n",
    "            sw_batch_size=1,\n",
    "            predictor=model,\n",
    "            overlap=0.5,\n",
    "        )\n",
    "\n",
    "    if VAL_AMP:\n",
    "        with torch.cuda.amp.autocast():\n",
    "            return _compute(input)\n",
    "    else:\n",
    "        return _compute(input)\n",
    "\n",
    "\n",
    "# use amp to accelerate training\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "# enable cuDNN benchmark\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8c1ad5-1839-41e7-a8e7-70afc278f674",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_metric = -1\n",
    "best_metric_epoch = -1\n",
    "best_metrics_epochs_and_time = [[], [], []]\n",
    "epoch_loss_values = []\n",
    "metric_values = []\n",
    "metric_values_1 = []\n",
    "metric_values_2 = []\n",
    "\n",
    "total_start = time.time()\n",
    "for epoch in range(max_epochs):\n",
    "    epoch_start = time.time()\n",
    "    print(\"-\" * 10)\n",
    "    print(f\"epoch {epoch + 1}/{max_epochs}\")\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    step = 0\n",
    "    for batch_data in train_loader:\n",
    "        step_start = time.time()\n",
    "        step += 1\n",
    "        inputs, labels = (\n",
    "            batch_data[\"image_petct\"].to(device),\n",
    "            batch_data[\"label\"].to(device),\n",
    "        )\n",
    "        # inputs = torch.concat([inputsct, inputspt], axis=1)\n",
    "        optimizer.zero_grad()\n",
    "        with torch.cuda.amp.autocast():\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_function(outputs, labels)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        epoch_loss += loss.item()\n",
    "        print(\n",
    "            f\"{step}/{len(train_ds) // train_loader.batch_size}\"\n",
    "            f\", train_loss: {loss.item():.4f}\"\n",
    "            f\", step time: {(time.time() - step_start):.4f}\"\n",
    "        )\n",
    "    lr_scheduler.step()\n",
    "    epoch_loss /= step\n",
    "    epoch_loss_values.append(epoch_loss)\n",
    "    print(f\"epoch {epoch + 1} average loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    if (epoch + 1) % val_interval == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for val_data in val_loader:\n",
    "                val_inputs, val_labels = (\n",
    "                    val_data[\"image_petct\"].to(device),\n",
    "                    val_data[\"label\"].to(device),\n",
    "                )\n",
    "                # val_inputs = torch.concat([val_inputsct, val_inputspt], axis=1)\n",
    "                val_outputs = inference(val_inputs)\n",
    "                val_outputs = [post_trans(i) for i in decollate_batch(val_outputs)]\n",
    "                dice_metric(y_pred=val_outputs, y=val_labels)\n",
    "                dice_metric_batch(y_pred=val_outputs, y=val_labels)\n",
    "\n",
    "            metric = dice_metric.aggregate().item()\n",
    "            metric_values.append(metric)\n",
    "            metric_batch = dice_metric_batch.aggregate()\n",
    "            metric_1 = metric_batch[0].item()\n",
    "            metric_values_1.append(metric_1)\n",
    "            metric_2 = metric_batch[1].item()\n",
    "            metric_values_2.append(metric_2)\n",
    "            dice_metric.reset()\n",
    "            dice_metric_batch.reset()\n",
    "\n",
    "            if metric > best_metric:\n",
    "                best_metric = metric\n",
    "                best_metric_epoch = epoch + 1\n",
    "                best_metrics_epochs_and_time[0].append(best_metric)\n",
    "                best_metrics_epochs_and_time[1].append(best_metric_epoch)\n",
    "                best_metrics_epochs_and_time[2].append(time.time() - total_start)\n",
    "                torch.save(\n",
    "                    model.state_dict(),\n",
    "                    os.path.join(root_dir, \"best_metric_model.pth\"),\n",
    "                )\n",
    "                print(\"saved new best metric model\")\n",
    "            print(\n",
    "                f\"current epoch: {epoch + 1} current mean dice: {metric:.4f}\"\n",
    "                f\" 1: {metric_1:.4f} 2: {metric_2:.4f}\"\n",
    "                f\"\\nbest mean dice: {best_metric:.4f}\"\n",
    "                f\" at epoch: {best_metric_epoch}\"\n",
    "            )\n",
    "    print(f\"time consuming of epoch {epoch + 1} is: {(time.time() - epoch_start):.4f}\")\n",
    "total_time = time.time() - total_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c7da3a-5e5c-46a3-bb46-132878adb015",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"train completed, best_metric: {best_metric:.4f} at epoch: {best_metric_epoch}, total time: {total_time}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f95a55-ffd2-4105-a744-723680331f94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(\"train\", (12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Epoch Average Loss\")\n",
    "x = [i + 1 for i in range(len(epoch_loss_values))]\n",
    "y = epoch_loss_values\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.plot(x, y, color=\"red\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Val Mean Dice\")\n",
    "x = [val_interval * (i + 1) for i in range(len(metric_values))]\n",
    "y = metric_values\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.plot(x, y, color=\"green\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(\"train\", (12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Val Mean Dice : 1\")\n",
    "x = [val_interval * (i + 1) for i in range(len(metric_values_1))]\n",
    "y = metric_values_1\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.plot(x, y, color=\"blue\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Val Mean Dice : 2\")\n",
    "x = [val_interval * (i + 1) for i in range(len(metric_values_2))]\n",
    "y = metric_values_2\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.plot(x, y, color=\"brown\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34fd202-1748-4fb4-b39b-98642ce56a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(os.path.join(root_dir, \"best_metric_model.pth\")))\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # select one image to evaluate and visualize the model output\n",
    "    val_inputct = val_ds[1][\"image\"].unsqueeze(0).to(device)\n",
    "    val_inputpt = val_ds[1][\"image2\"].unsqueeze(0).to(device)\n",
    "    val_input = torch.concat([val_inputct, val_inputpt], axis=1)\n",
    "    roi_size = (192, 192, 192)\n",
    "    sw_batch_size = 4\n",
    "    val_output = inference(val_input)\n",
    "    val_output = post_trans(val_output[0])\n",
    "    plt.figure(\"image\", (12, 6))\n",
    "    for i in range(2):\n",
    "        plt.subplot(1, 2, i + 1)\n",
    "        plt.title(f\"image channel {i}\")\n",
    "        plt.imshow(val_ds[1][\"image\"][i:, :, 70].detach().cpu(), cmap=\"gray\")\n",
    "    plt.show()\n",
    "    # visualize the 3 channels label corresponding to this image\n",
    "    plt.figure(\"label\", (12, 6))\n",
    "    for i in range(2):\n",
    "        plt.subplot(1, 2, i + 1)\n",
    "        plt.title(f\"label channel {i}\")\n",
    "        plt.imshow(val_ds[1][\"label\"][i, :, :, 70].detach().cpu())\n",
    "    plt.show()\n",
    "    # visualize the 3 channels model output corresponding to this image\n",
    "    plt.figure(\"output\", (12, 6))\n",
    "    for i in range(2):\n",
    "        plt.subplot(1, 2, i + 1)\n",
    "        plt.title(f\"output channel {i}\")\n",
    "        plt.imshow(val_output[i, :, :, 70].detach().cpu())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2bb456-9b43-4d56-a80d-37fd956bb3f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
